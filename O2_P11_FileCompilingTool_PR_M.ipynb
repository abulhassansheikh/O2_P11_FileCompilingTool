{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name: FileCompiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List necessary packages\n",
    "import glob as gl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name: MSCompiler\n",
    "\n",
    "#### What It Does: \n",
    "Extracts desired column/row values from all brand .csv mainsheet files.\n",
    "\n",
    "#### How To Use It: \n",
    "-\n",
    "\n",
    "#### How It Works:\n",
    "Indexes all brand folder, \n",
    "opens each folder, \n",
    "identifies files with \"main--\" prefix, \n",
    "opens them, \n",
    "subsets them as desired, \n",
    "joins appends data to \"CompiledMainSheet\" dataframe, \n",
    "exports \"CompiledMainSheet\" to specified location.\n",
    "\n",
    "#### Troubleshooting & Notes:\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check id duplicates of main--*.csv files and resolve any issues\n",
    "\n",
    "def MScheck():\n",
    "\n",
    "    AllBrandFolderPath = \"//192.168.2.32/GoogleDrive/Completed Magento Uploads (v 1.0)\"\n",
    "    AllBrandFolders = os.listdir(AllBrandFolderPath)\n",
    "\n",
    "    for b in range(len(AllBrandFolders)):\n",
    "\n",
    "        BrandPath = AllBrandFolderPath+\"/\"+AllBrandFolders[b]\n",
    "        BrandMSpath = BrandPath+\"/main--*.csv\"\n",
    "        BrandSSpath = BrandPath+\"/sub--*.csv\"\n",
    "        MSfilecount = len(gl.glob(BrandMSpath))\n",
    "        SSfilecount = len(gl.glob(BrandSSpath))\n",
    "        if MSfilecount != 1:\n",
    "            print(AllBrandFolders[b], \"MS\", MSfilecount)\n",
    "        #if SSfilecount != 1:\n",
    "            #print(AllBrandFolders[b], \"SS\", SSfilecount)\n",
    "\n",
    "MScheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile all brand mainsheet data into \"CompiledMainSheet\" dataframe\n",
    "\n",
    "def MScompile():\n",
    "    \n",
    "    low_memory=False\n",
    "    CompiledMainSheet = pd.DataFrame\n",
    "    AllColumns = []\n",
    "    \n",
    "    AllBrandFolderPath = \"//192.168.2.32/GoogleDrive/Completed Magento Uploads (v 1.0)\"\n",
    "    AllBrandFolders = os.listdir(AllBrandFolderPath)\n",
    "\n",
    "    for b in range(len(AllBrandFolders)):\n",
    "        print((len(AllBrandFolders)-b))\n",
    "        BrandPath= str(AllBrandFolderPath+\"/\"+AllBrandFolders[b])\n",
    "        BrandMSpath = BrandPath+\"/main--*.csv\"\n",
    "        filecount = len(gl.glob(BrandMSpath))\n",
    "        if filecount == 1:\n",
    "            MSFile = gl.glob(BrandMSpath)[0]     \n",
    "            BrandMS = list(map(str.strip, (pd.read_csv(MSFile, encoding='mac_roman').columns)))\n",
    "            AllColumns = list(set(AllColumns + BrandMS))\n",
    "    return(AllColumns)\n",
    "\n",
    "\n",
    "              \n",
    "len(AllColumns)\n",
    "cols = MScompile()\n",
    "b = 1\n",
    "\n",
    "AllColumns.strip()\n",
    "\n",
    "type(AllColumns)\n",
    "CompiledMainSheet = CompiledMainSheet.append(BrandMS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pull out tire and wheel filters from all subsheets\n",
    "\n",
    "WheelDataAll = pd.DataFrame(columns = [\"internal_sku\",\"wheel_diameter_pd\",\"wheel_width_pd\",\"bolt_pattern_pd\",\"offset_pd\",\"center_bore_pd\", \"brand\"])\n",
    "TireDataAll = pd.DataFrame(columns = [\"internal_sku\", \"tire_width_pd\", \"tire_profile_pd\", \"tire_wheel_dia_pd\", \"brand\"])\n",
    "    \n",
    "    \n",
    "AllBrandFolderPath = \"//192.168.2.32/GoogleDrive/Completed Magento Uploads (v 1.0)\"\n",
    "AllBrandFolders = os.listdir(AllBrandFolderPath)\n",
    "\n",
    "for b in range(len(AllBrandFolders)):\n",
    "    print((len(AllBrandFolders)-b))\n",
    "    Brand = AllBrandFolders[b]\n",
    "    BrandPath= str(AllBrandFolderPath+\"/\"+Brand)\n",
    "    BrandSSpath = BrandPath+\"/sub--*.csv\"\n",
    "    filecount = len(gl.glob(BrandSSpath))\n",
    "    if filecount >= 1:\n",
    "        for f in range(filecount):\n",
    "            SSFile = gl.glob(BrandSSpath)[f]\n",
    "            SSfile = pd.read_csv(SSFile, encoding='mac_roman')\n",
    "            BrandSS = list(map(str.strip, SSfile.columns))\n",
    "            SSfile.columns = BrandSS\n",
    "\n",
    "            if ('tire_width_pd' in BrandSS) == True:\n",
    "                TireData = (SSfile[(~pd.isnull(SSfile.tire_width_pd)) | (~pd.isnull(SSfile.tire_profile_pd)) \n",
    "                                   | (~pd.isnull(SSfile.tire_wheel_dia_pd))]\n",
    "                            [[\"internal_sku\", \"tire_width_pd\", \"tire_profile_pd\", \"tire_wheel_dia_pd\"]])\n",
    "                TireData[\"brand\"] = Brand\n",
    "                TireDataAll = TireDataAll.append(TireData, ignore_index=True)\n",
    "\n",
    "            if ('wheel_diameter_pd' in BrandSS) == True:\n",
    "                WheelData = (SSfile[(~pd.isnull(SSfile.wheel_diameter_pd)) | (~pd.isnull(SSfile.wheel_width_pd))\n",
    "                                    | (~pd.isnull(SSfile.bolt_pattern_pd)) | (~pd.isnull(SSfile.offset_pd)) \n",
    "                                    | (~pd.isnull(SSfile.center_bore_pd))]\n",
    "                            [[\"internal_sku\",\"wheel_diameter_pd\",\"wheel_width_pd\",\"bolt_pattern_pd\",\"offset_pd\",\"center_bore_pd\"]])\n",
    "                WheelData[\"brand\"] = Brand\n",
    "                WheelDataAll = WheelDataAll.append(WheelData, ignore_index=True)\n",
    "\n",
    "                \n",
    "WheelDataAll.to_csv (r'\\\\192.168.2.32\\Group\\Data Team\\Abul\\3. Final Folder\\WheelDataAll.csv', index = None, header=True) \n",
    "TireDataAll.to_csv (r'\\\\192.168.2.32\\Group\\Data Team\\Abul\\3. Final Folder\\TireDataAll.csv', index = None, header=True) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify all Mainsheet with column meta_title that has \"Canada\" in string in any row\n",
    "\n",
    "AllBrandFolderPath = \"//192.168.2.32/GoogleDrive/Completed Magento Uploads (v 1.0)\"\n",
    "AllBrandFolders = os.listdir(AllBrandFolderPath)\n",
    "\n",
    "for b in range(len(AllBrandFolders)):\n",
    "    #print((len(AllBrandFolders)-b))\n",
    "    Brand = AllBrandFolders[b]\n",
    "    BrandPath= str(AllBrandFolderPath+\"/\"+Brand)\n",
    "    BrandMSpath = BrandPath+\"/main--*.csv\"\n",
    "    filecount = len(gl.glob(BrandMSpath))\n",
    "    if filecount == 1:\n",
    "        MSFilepath = gl.glob(BrandMSpath)[0]     \n",
    "        MSFile = pd.read_csv(MSFilepath, encoding='mac_roman')\n",
    "        BrandMS = list(map(str.strip, MSFile.columns))\n",
    "        MSFile.columns = BrandMS          \n",
    "        C = len([i for i in MSFile.meta_title.astype(str).unique() if \"Canada\" in i])\n",
    "        L = len([i for i in MSFile.meta_title.astype(str).unique() if \"canada\" in i])\n",
    "        t = C+L\n",
    "        if t != 0:\n",
    "            print(\"*\",Brand)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile Category and Image data\n",
    "AllBrandFolderPath = \"//192.168.2.32/GoogleDrive/Completed Magento Uploads (v 1.0)\"\n",
    "AllBrandFolders = os.listdir(AllBrandFolderPath)\n",
    "\n",
    "CompiledMainSheet = pd.DataFrame(columns = [\"categories\"])\n",
    "TargetCol = [\"categories\", \"image1\",\"image2\",\"image3\",\"image4\"]\n",
    "\n",
    "for b in range(len(AllBrandFolders)):\n",
    "    print((len(AllBrandFolders)-b))\n",
    "    Brand = AllBrandFolders[b];     print(Brand)\n",
    "    BrandPath= str(AllBrandFolderPath+\"/\"+Brand)\n",
    "    BrandMSpath = BrandPath+\"/main--*.csv\"\n",
    "    filecount = len(gl.glob(BrandMSpath))\n",
    "\n",
    "    if filecount == 1:\n",
    "        MSFile = gl.glob(BrandMSpath)[0]     \n",
    "        BrandMS = pd.read_csv(MSFile, encoding='mac_roman')\n",
    "        BrandMS.columns = map(str.strip, list(BrandMS.columns))\n",
    "        BrandMS = BrandMS[TargetCol]\n",
    "        CompiledMainSheet = CompiledMainSheet.append(BrandMS, ignore_index=True)\n",
    "\n",
    "        \n",
    "Catdf   = CompiledMainSheet[~CompiledMainSheet[\"categories\"].isna()]\n",
    "Catdf_1 = Catdf[[\"categories\", \"image1\"]]\n",
    "Catdf_2 = Catdf[[\"categories\", \"image2\"]]\n",
    "Catdf_3 = Catdf[[\"categories\", \"image3\"]]\n",
    "Catdf_4 = Catdf[[\"categories\", \"image4\"]]\n",
    "\n",
    "Catdf_1.columns = [\"categories\", \"image\"]\n",
    "Catdf_2.columns = [\"categories\", \"image\"]\n",
    "Catdf_3.columns = [\"categories\", \"image\"]\n",
    "Catdf_4.columns = [\"categories\", \"image\"]\n",
    "\n",
    "Catdf_Pooled = Catdf_1.append(Catdf_2)\n",
    "Catdf_Pooled = Catdf_Pooled.append(Catdf_3)\n",
    "Catdf_Pooled = Catdf_Pooled.append(Catdf_4)\n",
    "\n",
    "#Do not want NA images\n",
    "Catdf_Pooled = Catdf_Pooled[~Catdf_Pooled[\"image\"].isna()]\n",
    "\n",
    "#Do not want single image being mapped to multiple unique categories\n",
    "Catdf_Pooled[\"Comb\"] = Catdf_Pooled[\"image\"]+\"~~\"+Catdf_Pooled[\"categories\"]\n",
    "CatComb = pd.DataFrame(list(Catdf_Pooled[\"Comb\"].unique()))\n",
    "CatComb.columns = [\"Comb\"]\n",
    "Catdf_Pooled = pd.DataFrame([re.split(\"~~\", c) for c in CatComb[\"Comb\"]])\n",
    "Catdf_Pooled.columns = [\"image\", \"categories\"]\n",
    "\n",
    "#Identify images with a single unique Category\n",
    "ImageReF = pd.DataFrame(Catdf_Pooled.groupby(\"image\").agg(\"count\")[\"categories\"].sort_values().reset_index())\n",
    "ImageReF = ImageReF[ImageReF[\"categories\"] == 1][[\"image\"]]\n",
    "\n",
    "#ID Categories corrisponding to images to a single category\n",
    "CatRef = pd.DataFrame({\"categories\":Catdf_Pooled[Catdf_Pooled[\"image\"].isin(ImageReF[\"image\"])][\"categories\"]\n",
    "                       .dropna().unique().tolist()})\n",
    "\n",
    "for c in range(len(CatRef)):\n",
    "    cat = CatRef.loc[c, \"categories\"]\n",
    "    BrokenCat = re.split(\";\", cat)\n",
    "    count = len(BrokenCat)\n",
    "    BrokenCat = [b for b in BrokenCat if b.find(\"Brands/\") == -1]\n",
    "    Dub = 0\n",
    "    L1_dub = 0\n",
    "\n",
    "    if len(BrokenCat)>0:\n",
    "\n",
    "        BrokenLevel = pd.DataFrame([re.split(\"/\", l) for l in BrokenCat])\n",
    "        L1_len = len(BrokenLevel.iloc[:,0].dropna().unique().tolist())\n",
    "\n",
    "        if L1_len==1: L1_dub = 1\n",
    "        if L1_len >=2 and len(BrokenLevel)>2:\n",
    "            L1_subset = BrokenLevel.groupby(0).agg(\"count\").sort_values(1, ascending = False).reset_index()\n",
    "            BrokenLevel = BrokenLevel[BrokenLevel.loc[:,0] == L1_subset.iloc[0,0]]\n",
    "            L1_dub = 1\n",
    "                                      \n",
    "        if L1_dub==1:\n",
    "            if len(BrokenLevel.columns) ==3:\n",
    "                BrokenLevel.columns = [\"L1\", \"L2\", \"L3\"]\n",
    "\n",
    "                L1 = BrokenLevel[\"L1\"].unique()\n",
    "                L2 = BrokenLevel[\"L2\"].unique()\n",
    "                L3 = BrokenLevel[\"L3\"].unique()\n",
    "                if (len(L1)==1 and len(L2)==1 and len(L3)==1): levCount = 3\n",
    "                if (len(L1)==1 and len(L2)==1 and len(L3)!=1): levCount = 2\n",
    "                if (len(L1)==1 and len(L2)!=1 and len(L3)!=1): levCount = 1\n",
    "                if (len(L1)==1 and len(L2)!=1 and len(L3)==1): levCount = 1; Dub = 1\n",
    "\n",
    "            elif len(BrokenLevel.columns) ==2:\n",
    "                BrokenLevel.columns = [\"L1\", \"L2\"]\n",
    "                L1 = BrokenLevel[\"L1\"].unique()\n",
    "                L2 = BrokenLevel[\"L2\"].unique()\n",
    "                if (len(L1)==1 and len(L2)==1 ): levCount = 2\n",
    "                if (len(L1)==1 and len(L2)!=1 ): levCount = 1\n",
    "\n",
    "            elif len(BrokenLevel.columns) ==1:\n",
    "                BrokenLevel.columns = [\"L1\"]\n",
    "                L1 = BrokenLevel[\"L1\"].unique()\n",
    "                if (len(L1)==1 ): levCount = 1\n",
    "\n",
    "            if levCount == 3:\n",
    "                CatRef.loc[c, \"count\"] = count\n",
    "                CatRef.loc[c, \"levCount\"] = levCount\n",
    "                CatRef.loc[c, \"L1\"] = L1\n",
    "                CatRef.loc[c, \"L3\"] = L3\n",
    "\n",
    "                if Dub == 0: CatRef.loc[c, \"L2\"] = L2\n",
    "                if Dub == 1: CatRef.loc[c, \"L2\"] = sorted(L2,key=len)[0]\n",
    "\n",
    "            elif levCount == 2:\n",
    "                CatRef.loc[c, \"count\"] = count\n",
    "                CatRef.loc[c, \"levCount\"] = levCount\n",
    "                CatRef.loc[c, \"L1\"] = L1\n",
    "                CatRef.loc[c, \"L2\"] = L2\n",
    "            elif levCount == 1:\n",
    "                CatRef.loc[c, \"count\"] = count\n",
    "                CatRef.loc[c, \"levCount\"] = levCount\n",
    "                CatRef.loc[c, \"L1\"] = L1\n",
    "\n",
    "Catdf_Final = Catdf_Pooled.merge(CatRef, on = \"categories\", how = \"outer\")\n",
    "Catdf_Final = Catdf_Final[~Catdf_Final[\"L1\"].isna()]\n",
    "\n",
    "#Test if there is one-to-one image/category combos\n",
    "len((Catdf_Final[\"image\"]+\"~~\"+Catdf_Final[\"categories\"]).unique())\n",
    "len(Catdf_Final[\"image\"])\n",
    "\n",
    "Catdf_Final.to_csv(r'\\\\192.168.2.32\\Group\\Data Team\\Abul\\Catdf_Final.csv', index = None, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zero-sale skus data compile\n",
    "\n",
    "AllBrandFolderPath = \"//192.168.2.32/GoogleDrive/Completed Magento Uploads (v 1.0)\"\n",
    "AllBrandFolders = os.listdir(AllBrandFolderPath)\n",
    "\n",
    "#NonSoldSkus=pd.read_csv(\"//192.168.2.32/Group/Data Team/Brand_Update_Location/5_R_Brand_Reference_Files/NonSoldSkus.csv\", encoding='utf-8')\n",
    "#NonSoldSkus = list(NonSoldSkus[\"internal_sku\"])\n",
    "\n",
    "CompiledMainSheet = pd.DataFrame(columns = [\"categories\"])\n",
    "TargetCol = [\"internal_sku\", \"ca_cost\",\"na_ca_shipping\",\"part_type_filter\",\"attribute_set\"]\n",
    "#TargetCol = [\"categories\", \"image1\",\"image2\",\"image3\",\"image4\"]\n",
    "#TargetCol = [\"internal_sku\", \"sku\", \"ca_price\", \"ca_retail_price\", \"ca_cost\", \"ca_jobber_price\", \"na_ca_shipping\", \"na_supplier\"]\n",
    "\n",
    "for b in range(len(AllBrandFolders)):\n",
    "    print((len(AllBrandFolders)-b))\n",
    "    Brand = AllBrandFolders[b];     print(Brand)\n",
    "    BrandPath= str(AllBrandFolderPath+\"/\"+Brand)\n",
    "    BrandMSpath = BrandPath+\"/main--*.csv\"\n",
    "    filecount = len(gl.glob(BrandMSpath))\n",
    "\n",
    "    if filecount == 1:\n",
    "        MSFile = gl.glob(BrandMSpath)[0]     \n",
    "        BrandMS = pd.read_csv(MSFile, encoding='mac_roman')\n",
    "        BrandMS.columns = map(str.strip, list(BrandMS.columns))\n",
    "        BrandMS = BrandMS[(BrandMS[\"type\"] == \"simple\") & (BrandMS[\"delete\"] == \"N\")]\n",
    "        BrandMS = BrandMS[TargetCol]\n",
    "\n",
    "        #BrandMS = BrandMS[BrandMS[\"internal_sku\"].isin(NonSoldSkus)]\n",
    "        CompiledMainSheet = CompiledMainSheet.append(BrandMS, ignore_index=True)\n",
    "\n",
    "            \n",
    "(CompiledMainSheet.to_csv(r'\\\\192.168.2.32\\Group\\Data Team\\Abul\\CompiledMainSheet.csv', index = None, header=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PriceAnalysis = CompiledMainSheet\n",
    "\n",
    "PriceAnalysis = (PriceAnalysis[~(PriceAnalysis[\"na_ca_shipping\"].isna()) &\n",
    "                                     ~(PriceAnalysis[\"ca_cost\"].isna()) &\n",
    "                                    (PriceAnalysis[\"ca_cost\"] != \"TBA\")])\n",
    "\n",
    "PriceAnalysis[\"price\"] = (PriceAnalysis[\"na_ca_shipping\"].astype(float) + PriceAnalysis[\"ca_cost\"].astype(float)) \n",
    "PriceAnalysis[\"ShipCostRatio\"] = round(PriceAnalysis[\"na_ca_shipping\"].astype(float) / PriceAnalysis[\"price\"], 1)\n",
    "\n",
    "PriceAnalysis[PriceAnalysis[\"ShipCostRatio\"] >= .5]\n",
    "PriceAnalysis.groupby(\"ShipCostRatio\").agg(\"count\").reset_index()[[\"ShipCostRatio\", \"na_ca_shipping\"]]\n",
    "\n",
    "A = PriceAnalysis.groupby(\"part_type_filter\")[\"ShipCostRatio\"].agg(\"mean\").reset_index()#[[\"ShipCostRatio\", \"na_ca_shipping\"]]\n",
    "\n",
    "(A.to_csv(r'\\\\192.168.2.32\\Group\\Data Team\\Abul\\4. Temp Folder\\PTshippingPercent.csv', index = None, header=True))\n",
    "\n",
    "(CompiledMainSheet.to_csv(r'\\\\192.168.2.32\\Group\\Data Team\\Abul\\CompiledMainSheet.csv', index = None, header=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllBrandFolderPath = \"//192.168.2.32/GoogleDrive/Completed Magento Uploads (v 1.0)\"\n",
    "AllBrandFolders = os.listdir(AllBrandFolderPath)\n",
    "\n",
    "Ref = pd.DataFrame({\"brands\":AllBrandFolders, \"MS\":0, \"SS\":0})\n",
    "\n",
    "for b in range(len(Ref)):\n",
    "    Brand = Ref.loc[b, \"brands\"]\n",
    "    BrandPath= str(AllBrandFolderPath+\"/\"+Brand)\n",
    "    BrandMSpath = BrandPath+\"/*main--*.csv\"\n",
    "    BrandSSpath = BrandPath+\"/*sub*.csv\"\n",
    "\n",
    "    Ref.loc[b, \"MS\"] = len(gl.glob(BrandMSpath))\n",
    "    Ref.loc[b, \"SS\"] = len(gl.glob(BrandSSpath))\n",
    "    \n",
    "(Ref.to_csv(r'\\\\192.168.2.32\\Group\\Data Team\\Abul\\Ref.csv', index = None, header=True))\n",
    "\n",
    "\n",
    "    print(\"CSV Count:\",filecountMS, filecountSS, \" Brand:\", Brand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zero-sale skus data compile\n",
    "\n",
    "AllBrandFolderPath = \"//192.168.2.32/GoogleDrive/Completed Magento Uploads (v 1.0)\"\n",
    "AllBrandFolders = os.listdir(AllBrandFolderPath)\n",
    "\n",
    "\n",
    "TargetPT =[ \t\n",
    "            \"Floor Mats\", \t\n",
    "            \"Brake Kits\", \t\n",
    "            \"Performance Chips & Programmers\", \t\n",
    "            \"Shocks & Struts\"]#, \t\n",
    "            #\"Coil-Overs\", \t\n",
    "            #\"Suspension Lift Kits\", \t\n",
    "            #\"Wheels\", \t\n",
    "            #\"Headlight Assemblies\", \t\n",
    "            #\"Nerf Bars & Step Bars\"]\n",
    "\n",
    "for b in range(192, len(AllBrandFolders)):\n",
    "    print((len(AllBrandFolders)-b))\n",
    "    Brand = AllBrandFolders[b];     print(Brand)\n",
    "    BrandPath= str(AllBrandFolderPath+\"/\"+Brand)\n",
    "    BrandMSpath = BrandPath+\"/main--*.csv\"\n",
    "    BrandSSpath = BrandPath+\"/sub--*.csv\"\n",
    "    filecount = len(gl.glob(BrandMSpath))\n",
    "    filecountSS = len(gl.glob(BrandSSpath))\n",
    "\n",
    "    PTFolderPath = \"//192.168.2.32/GoogleDrive/PT_mainsheet\"\n",
    "    AllPTfiles = os.listdir(PTFolderPath)\n",
    "\n",
    "    if filecount == 1:\n",
    "        MSFile = gl.glob(BrandMSpath)[0]\n",
    "        BrandMS = pd.read_csv(MSFile, encoding='mac_roman')\n",
    "        BrandMS.columns = [c.strip() for c in BrandMS.columns]\n",
    "\n",
    "        brandPT = BrandMS[\"part_type_filter\"].dropna().unique().tolist()\n",
    "        brandPT = [c.strip() for c in brandPT]\n",
    "        brandPT = [p for p in brandPT if p in TargetPT]\n",
    "\n",
    "        if len(brandPT) != 0:\n",
    "            for p in range(len(brandPT)):\n",
    "                pt = brandPT[p]; print(pt)\n",
    "                pt_new = re.sub(\"-\", \"_\", pt)\n",
    "                pt_new = re.sub(\"&\", \"and\", pt_new)\n",
    "                pt_new = re.sub(\";\", \"_or_\", pt_new)\n",
    "                pt_new = re.sub(\"/\", \"_\", pt_new)\n",
    "                pt_new = re.sub(\" \", \"_\", pt_new)\n",
    "                pt_new = re.sub(\"__\", \"_\", pt_new)\n",
    "                pt_new = re.sub(\"__\", \"_\", pt_new)\n",
    "\n",
    "                pt_file = \"main--\"+pt_new+\".csv\"\n",
    "                pt_path = PTFolderPath+\"/\"+pt_file\n",
    "                pt_fileSS = \"sub--\"+pt_new+\".csv\"\n",
    "                pt_pathSS = PTFolderPath+\"/\"+pt_fileSS\n",
    "\n",
    "                #Mainsheet Sku Pull\n",
    "                PtSubset = BrandMS[BrandMS[\"part_type_filter\"] == pt]\n",
    "                PtSubset = PtSubset.dropna(axis=1,how='all')\n",
    "                PTintSku = PtSubset[\"internal_sku\"].dropna().unique().tolist()\n",
    "\n",
    "                #Series Pull\n",
    "                Pt_serieslist = list(PtSubset[\"series_parent\"].unique())\n",
    "                SeriesSubset = BrandMS[(BrandMS[\"type\"]== \"series\") &\n",
    "                                       (BrandMS[\"internal_sku\"].isin(Pt_serieslist))]\n",
    "                SeriesSubset = SeriesSubset.dropna(axis=1,how='all')\n",
    "\n",
    "                #Append sku with series\n",
    "                finalsubset = PtSubset.append(SeriesSubset,sort=False)\n",
    "\n",
    "                if (pt_file in AllPTfiles) == True:\n",
    "\n",
    "                    pt_fileMS_test = PTFolderPath+\"/\"+\"main--\"+pt_new+\"*.csv\"\n",
    "                    num = str(len(gl.glob(pt_fileMS_test)))\n",
    "                    LatestMSfile = gl.glob(pt_fileMS_test)[-1]\n",
    "                    MSfileSize = os.path.getsize(LatestMSfile)*0.001\n",
    "\n",
    "                    if MSfileSize <= 100000:\n",
    "                        Old_PTfile=pd.read_csv(LatestMSfile, encoding='utf-8')\n",
    "                        New_PTfile = Old_PTfile.append(finalsubset, sort=False)\n",
    "                        New_PTfile.to_csv(LatestMSfile, index = None, header=True,  encoding='utf-8')\n",
    "\n",
    "                    else:\n",
    "                        newMSPath = PTFolderPath+\"/\"+\"main--\"+pt_new+\"_\"+num+\".csv\"\n",
    "                        finalsubset.to_csv(newMSPath, index = None, header=True,  encoding='utf-8')\n",
    "\n",
    "                else:\n",
    "                    finalsubset.to_csv(pt_path, index = None, header=True,  encoding='utf-8')\n",
    "\n",
    "\n",
    "               #Pull out Subsheet \n",
    "                if filecountSS >= 1:              \n",
    "                    SSFile = gl.glob(BrandSSpath)   \n",
    "                    BrandSS = pd.DataFrame()\n",
    "                    \n",
    "                    for s in range(len(SSFile)):\n",
    "                        BrandSS_single = pd.read_csv(SSFile[s], encoding='mac_roman')\n",
    "                        BrandSS_single.columns = [c.strip() for c in BrandSS_single.columns]\n",
    "                        BrandSS = BrandSS.append(BrandSS_single, sort=False)\n",
    "\n",
    "                    #Subset Subsheet\n",
    "                    PTSubSheet = BrandSS[BrandSS[\"internal_sku\"].isin(PTintSku)]\n",
    "                    PTSubSheet = PTSubSheet.dropna(axis=1,how='all')\n",
    "                                                                            \n",
    "                    if (pt_fileSS in AllPTfiles) == True:\n",
    "\n",
    "                        pt_fileSS_test = PTFolderPath+\"/\"+\"sub--\"+pt_new+\"*.csv\"\n",
    "                        num = str(len(gl.glob(pt_fileSS_test)))\n",
    "                        LatestSSfile = gl.glob(pt_fileSS_test)[-1]\n",
    "                        SSfileSize = os.path.getsize(LatestSSfile)*0.001\n",
    "\n",
    "                        if SSfileSize <= 100000:\n",
    "                            Old_SSfile=pd.read_csv(LatestSSfile, encoding='utf-8')\n",
    "                            New_SSfile = Old_SSfile.append(PTSubSheet, sort=False)\n",
    "                            New_SSfile.to_csv(LatestSSfile, index = None, header=True,  encoding='utf-8')\n",
    "\n",
    "                        else:\n",
    "                            newSSPath = PTFolderPath+\"/\"+\"sub--\"+pt_new+\"_\"+num+\".csv\"\n",
    "                            PTSubSheet.to_csv(newSSPath, index = None, header=True,  encoding='utf-8')\n",
    "\n",
    "                    else:\n",
    "                        PTSubSheet.to_csv(pt_pathSS, index = None, header=True,  encoding='utf-8')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load MasterDS_R file\n",
    "MasterDS=pd.read_csv(\"//192.168.2.32/Group/Data Team/Recommender_System_Location/1_Reference_Files/MasterDS.csv\", encoding='utf-8')\n",
    "\n",
    "list(MasterDS.columns)\n",
    "\n",
    "ShippingData_R = MasterDS[[\"Order Date\", \"Product Cost (CAD)\", \"Shipping Cost (CAD)\", \"Net Retail Price (CAD)\", \"Province\", \"part_type_filter\"]]\n",
    "ShippingData_R[\"Shipping Cost (CAD)\"] = [float(  (str(s).replace(\"$\", \"\")).replace(\",\", \"\")  ) for s in ShippingData_R[\"Shipping Cost (CAD)\"] ]\n",
    "ShippingData_R[\"Net Retail Price (CAD)\"] = [float(  (str(s).replace(\"$\", \"\")).replace(\",\", \"\")  ) for s in ShippingData_R[\"Net Retail Price (CAD)\"] ]\n",
    "\n",
    "ShippingData  = ShippingData_R[~(ShippingData_R[\"Product Cost (CAD)\"].isna()) &\n",
    "                               ~(ShippingData_R[\"Shipping Cost (CAD)\"].isna()) &\n",
    "                               ~(ShippingData_R[\"Net Retail Price (CAD)\"].isna()) &\n",
    "\n",
    "                               ~(ShippingData_R[\"part_type_filter\"].isna()) &\n",
    "                               ~(ShippingData_R[\"Product Cost (CAD)\"] == 0) &\n",
    "                               ~(ShippingData_R[\"Shipping Cost (CAD)\"] == 0) &\n",
    "                               ~(ShippingData_R[\"Net Retail Price (CAD)\"] == 0) \n",
    "                              ]\n",
    "\n",
    "\n",
    "ShippingData[\"price\"] = (ShippingData[\"Shipping Cost (CAD)\"].astype(float) + ShippingData[\"Product Cost (CAD)\"].astype(float)) \n",
    "ShippingData[\"ShipCostRatio\"] = round(ShippingData[\"Shipping Cost (CAD)\"].astype(float) / ShippingData[\"price\"], 1)\n",
    "\n",
    "ShippingData['Order_Date'] = pd.to_datetime(ShippingData['Order Date'], format= \"%d-%b-%y\")\n",
    "ShippingData['OD_Year'] = pd.to_numeric(ShippingData['Order_Date'].dt.strftime('%Y'))\n",
    "\n",
    "A = ShippingData.groupby(\"part_type_filter\")[\"ShipCostRatio\"].agg(\"mean\").reset_index()#[[\"ShipCostRatio\", \"na_ca_shipping\"]]\n",
    "\n",
    "B = ShippingData.groupby([\"part_type_filter\", \"Province\"]).agg(\"count\").reset_index()[[\"part_type_filter\", \"Province\", \"Order Date\"]]\n",
    "B = B.pivot(index='part_type_filter', columns='Province', values='Order Date').reset_index()\n",
    "\n",
    "C = ShippingData.groupby([\"part_type_filter\", \"OD_Year\"])[\"Net Retail Price (CAD)\"].agg(\"sum\").reset_index()[[\"part_type_filter\", \"OD_Year\", \"Net Retail Price (CAD)\"]]\n",
    "C = C.pivot(index='part_type_filter', columns='OD_Year', values='Net Retail Price (CAD)').reset_index()\n",
    "C.columns = [\"part_type_filter\", \"2015_Rev\", \"2016_Rev\", \"2017_Rev\", \"2018_Rev\", \"2019_Rev\"]\n",
    "\n",
    "Z = A.merge(B, on = \"part_type_filter\", how = \"outer\")\n",
    "Z = Z.merge(C, on = \"part_type_filter\", how = \"outer\")\n",
    "\n",
    "\n",
    "(Z.to_csv(r'\\\\192.168.2.32\\Group\\Data Team\\Abul\\4. Temp Folder\\PTshippingPerProv.csv', index = None, header=True))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
